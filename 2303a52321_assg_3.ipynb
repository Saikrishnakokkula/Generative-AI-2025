{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI8VYp089bGd",
        "outputId": "6a93d875-4cc9-4713-af63-39bbd2352c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged at x = 1.5377093794492703e-05 after 166 iterations\n",
            "The value of x at which f(x) is minimized is: 1.5377093794492703e-05\n"
          ]
        }
      ],
      "source": [
        "#assigment_3\n",
        "#1 question\n",
        "\n",
        "def f_prime(x):\n",
        "    # Derivative of the function f(x) = 5x^4 + 3x^2 + 10\n",
        "    return 20 * x**3 + 6 * x\n",
        "\n",
        "def gradient_descent(learning_rate=0.01, initial_x=1.0, tolerance=1e-6, max_iterations=10000):\n",
        "    x = initial_x\n",
        "    for i in range(max_iterations):\n",
        "        # Compute the gradient (derivative of f(x))\n",
        "        grad = f_prime(x)\n",
        "\n",
        "        # Update x using the gradient descent formula\n",
        "        x_new = x - learning_rate * grad\n",
        "\n",
        "        # If the change is small enough, stop the iteration (convergence criterion)\n",
        "        if abs(x_new - x) < tolerance:\n",
        "            print(f\"Converged at x = {x_new} after {i+1} iterations\")\n",
        "            return x_new\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "    print(f\"Reached max iterations. Last value of x: {x}\")\n",
        "    return x\n",
        "\n",
        "# Running the gradient descent algorithm\n",
        "minimum_x = gradient_descent()\n",
        "print(\"The value of x at which f(x) is minimized is:\", minimum_x)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2 question\n",
        "\n",
        "import math\n",
        "\n",
        "# Partial derivatives of the function g(x, y) = 3x^2 + 5e^(-y) + 10\n",
        "def g_prime_x(x, y):\n",
        "    # Partial derivative with respect to x: 6x\n",
        "    return 6 * x\n",
        "\n",
        "def g_prime_y(x, y):\n",
        "    # Partial derivative with respect to y: -5e^(-y)\n",
        "    return -5 * math.exp(-y)\n",
        "\n",
        "def gradient_descent(learning_rate=0.01, initial_x=1.0, initial_y=1.0, tolerance=1e-6, max_iterations=10000):\n",
        "    x = initial_x\n",
        "    y = initial_y\n",
        "    for i in range(max_iterations):\n",
        "        # Compute the gradients (partial derivatives of g(x, y))\n",
        "        grad_x = g_prime_x(x, y)\n",
        "        grad_y = g_prime_y(x, y)\n",
        "\n",
        "        # Update x and y using the gradient descent formula\n",
        "        x_new = x - learning_rate * grad_x\n",
        "        y_new = y - learning_rate * grad_y\n",
        "\n",
        "        # Check if the change is smaller than tolerance (convergence criterion)\n",
        "        if abs(x_new - x) < tolerance and abs(y_new - y) < tolerance:\n",
        "            print(f\"Converged at x = {x_new}, y = {y_new} after {i+1} iterations\")\n",
        "            return x_new, y_new\n",
        "\n",
        "        # Update x and y for the next iteration\n",
        "        x = x_new\n",
        "        y = y_new\n",
        "\n",
        "    print(f\"Reached max iterations. Last values of x, y: {x}, {y}\")\n",
        "    return x, y\n",
        "\n",
        "# Running the gradient descent algorithm\n",
        "minimum_x, minimum_y = gradient_descent()\n",
        "print(f\"The values of x and y that minimize g(x, y) are: x = {minimum_x}, y = {minimum_y}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6F2T72HpCIBB",
        "outputId": "001a81d1-eb7e-45d0-d65c-32cc246bb5f0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reached max iterations. Last values of x, y: 1.8990482403275175e-269, 6.220289812067548\n",
            "The values of x and y that minimize g(x, y) are: x = 1.8990482403275175e-269, y = 6.220289812067548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3 question\n",
        "\n",
        "import math\n",
        "\n",
        "# Derivative of the sigmoid function z(x) = 1 / (1 + e^(-x))\n",
        "def sigmoid_prime(x):\n",
        "    # Derivative: e^(-x) / (1 + e^(-x))^2\n",
        "    return math.exp(-x) / (1 + math.exp(-x))**2\n",
        "\n",
        "def gradient_descent(learning_rate=0.1, initial_x=1.0, tolerance=1e-6, max_iterations=10000):\n",
        "    x = initial_x\n",
        "    for i in range(max_iterations):\n",
        "        # Compute the gradient (derivative of z(x))\n",
        "        grad = sigmoid_prime(x)\n",
        "\n",
        "        # Update x using the gradient descent formula\n",
        "        x_new = x - learning_rate * grad\n",
        "\n",
        "        # Check if the change in x is smaller than tolerance (convergence criterion)\n",
        "        if abs(x_new - x) < tolerance:\n",
        "            print(f\"Converged at x = {x_new} after {i+1} iterations\")\n",
        "            return x_new\n",
        "\n",
        "        x = x_new\n",
        "\n",
        "    print(f\"Reached max iterations. Last value of x: {x}\")\n",
        "    return x\n",
        "\n",
        "# Running the gradient descent algorithm\n",
        "minimum_x = gradient_descent()\n",
        "print(\"The value of x at which z(x) is minimized is:\", minimum_x)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFXVLj5PCYsY",
        "outputId": "873ce0f2-842c-4e84-f7d9-87b2748b11d4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reached max iterations. Last value of x: -6.8897282615101805\n",
            "The value of x at which z(x) is minimized is: -6.8897282615101805\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4 question\n",
        "\n",
        "# Function to compute the Square Error\n",
        "def compute_square_error(X, Y, M, C):\n",
        "    SE = 0\n",
        "    for i in range(len(X)):\n",
        "        predicted_output = M * X[i] + C\n",
        "        SE += (Y[i] - predicted_output) ** 2\n",
        "    return SE\n",
        "\n",
        "# Derivative of the Square Error with respect to M\n",
        "def compute_gradient_M(X, Y, M, C):\n",
        "    grad_M = 0\n",
        "    for i in range(len(X)):\n",
        "        predicted_output = M * X[i] + C\n",
        "        grad_M += -2 * X[i] * (Y[i] - predicted_output)\n",
        "    return grad_M\n",
        "\n",
        "# Derivative of the Square Error with respect to C\n",
        "def compute_gradient_C(X, Y, M, C):\n",
        "    grad_C = 0\n",
        "    for i in range(len(X)):\n",
        "        predicted_output = M * X[i] + C\n",
        "        grad_C += -2 * (Y[i] - predicted_output)\n",
        "    return grad_C\n",
        "\n",
        "# Gradient Descent Algorithm to minimize Square Error\n",
        "def gradient_descent(X, Y, learning_rate=0.01, initial_M=0, initial_C=0, tolerance=1e-6, max_iterations=10000):\n",
        "    M = initial_M\n",
        "    C = initial_C\n",
        "    for i in range(max_iterations):\n",
        "        # Compute gradients\n",
        "        grad_M = compute_gradient_M(X, Y, M, C)\n",
        "        grad_C = compute_gradient_C(X, Y, M, C)\n",
        "\n",
        "        # Update parameters M and C\n",
        "        M_new = M - learning_rate * grad_M\n",
        "        C_new = C - learning_rate * grad_C\n",
        "\n",
        "        # Check for convergence (if the changes are smaller than tolerance)\n",
        "        if abs(M_new - M) < tolerance and abs(C_new - C) < tolerance:\n",
        "            print(f\"Converged after {i+1} iterations\")\n",
        "            return M_new, C_new\n",
        "\n",
        "        M = M_new\n",
        "        C = C_new\n",
        "\n",
        "    print(f\"Reached max iterations. Last values of M, C: {M}, {C}\")\n",
        "    return M, C\n",
        "\n",
        "# Example data (X: input values, Y: expected output values)\n",
        "X = [1, 2, 3, 4, 5]\n",
        "Y = [2, 4, 6, 8, 10]  # For this example, the expected output is a linear function Y = 2 * X\n",
        "\n",
        "# Running the gradient descent to find optimal values of M and C\n",
        "optimal_M, optimal_C = gradient_descent(X, Y)\n",
        "print(f\"The optimal values of M and C are: M = {optimal_M}, C = {optimal_C}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwirH_ZnC6Fw",
        "outputId": "16f2cd1e-277e-48e1-a731-12509c34c4b9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converged after 534 iterations\n",
            "The optimal values of M and C are: M = 1.9999841557626623, C = 5.720272413883778e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UNNuA1z4FjFm"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}